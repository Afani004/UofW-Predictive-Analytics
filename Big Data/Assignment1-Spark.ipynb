{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use the walmart_stock.csv file to Answer and complete the  tasks below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start a simple Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession is an entry point to PySpark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import * \n",
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a variable 'spark' to store the SparkSession instance\n",
    "spark = SparkSession.builder.appName('Assignment1').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Walmart Stock CSV File into a data frame called df, have Spark infer the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the Walmart dataset\n",
    "dframe = spark.read.csv('walmart_stock.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Adj Close']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the Walmart dataset\n",
    "dframe.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does the Schema look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: timestamp (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# viewing the dataset schema\n",
    "dframe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|               Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "|2012-01-03 00:00:00|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|\n",
      "|2012-01-04 00:00:00|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475|\n",
      "|2012-01-05 00:00:00|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|\n",
      "|2012-01-06 00:00:00|         59.419998|59.450001|58.869999|              59.0| 8069400|          51.45922|\n",
      "|2012-01-09 00:00:00|         59.029999|59.549999|58.919998|             59.18| 6679300|51.616215000000004|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# printing out the 5 rows using spark sql\n",
    "sql = dframe.createOrReplaceTempView('walmart_stock')\n",
    "sql = spark.sql('SELECT *  FROM walmart_stock LIMIT 5')\n",
    "sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use describe() to learn about the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Open: string, High: string, Low: string, Close: string, Volume: string, Adj Close: string]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describing the dataframe\n",
    "dframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- summary: string (nullable = true)\n",
      " |-- Open: string (nullable = true)\n",
      " |-- High: string (nullable = true)\n",
      " |-- Low: string (nullable = true)\n",
      " |-- Close: string (nullable = true)\n",
      " |-- Volume: string (nullable = true)\n",
      " |-- Adj Close: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We store the dataFrame discibtion in a new dataFrame called 'dfDescribe'\n",
    "dfDescribe = dframe.describe()\n",
    "# Notice the data types in dfDescribe\n",
    "dfDescribe.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format the numbers in dfDescribe to just show up two decimal places\n",
    "#### If you get stuck here, you can continue the rest of the assignment and try this part later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+------+------+----------+---------+\n",
      "|summary|  Open|  High|   Low| Close|    Volume|Adj Close|\n",
      "+-------+------+------+------+------+----------+---------+\n",
      "|  count|1258.0|1258.0|1258.0|1258.0|    1258.0|   1258.0|\n",
      "|   mean| 72.36| 72.84| 71.92| 72.39|8222093.48|    67.24|\n",
      "| stddev|  6.77|  6.77|  6.74|  6.76|4519780.84|     6.72|\n",
      "|    min| 56.39| 57.06|  56.3| 56.42| 2094900.0|    50.36|\n",
      "|    max|  90.8| 90.97| 89.25| 90.47| 8.08981E7|    84.91|\n",
      "+-------+------+------+------+------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using for loop to round the columns to 2two decimal places\n",
    "for v in dfDescribe.columns[1:]:\n",
    "    dfDescribe = dfDescribe.withColumn(v, func.round(v, 2))\n",
    "\n",
    "dfDescribe.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new dataframe with a column called Ratio that is the ratio of the High Price versus volume of stock traded for a day. Print the first five rows in that column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+------------------+\n",
      "|               Date|              Open|     High|      Low|             Close|  Volume|         Adj Close|             Ratio|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+------------------+\n",
      "|2012-01-03 00:00:00|         59.970001|61.060001|59.869999|         60.330002|12668800|52.619234999999996|207481.16266817617|\n",
      "|2012-01-04 00:00:00|60.209998999999996|60.349998|59.470001|59.709998999999996| 9593300|         52.078475| 158961.0657485026|\n",
      "|2012-01-05 00:00:00|         59.349998|59.619999|58.369999|         59.419998|12768200|         51.825539|214159.68155249383|\n",
      "|2012-01-06 00:00:00|         59.419998|59.450001|58.869999|              59.0| 8069400|          51.45922|135734.22816258657|\n",
      "|2012-01-09 00:00:00|         59.029999|59.549999|58.919998|             59.18| 6679300|51.616215000000004|112162.89021264299|\n",
      "+-------------------+------------------+---------+---------+------------------+--------+------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataframe with a column called Ratio \n",
    "# which shows the ratio of High Price versus volume of stock traded for a day\n",
    "dframe_ratio = dframe.withColumn('Ratio', (dframe['Volume'] / dframe['High']))\n",
    "dframe_ratio.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What day had the Peak in the column 'High' in Price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|           Peak_day|\n",
      "+-------------------+\n",
      "|2015-01-13 00:00:00|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sql = dframe.createOrReplaceTempView('walmart_stock')\n",
    "sql = spark.sql('SELECT max_by(Date, High) Peak_day FROM walmart_stock')\n",
    "sql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the mean of the Close column (print it as a float number)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|Average(Close)|\n",
      "+--------------+\n",
      "|      72.38845|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the mean of the Close column\n",
    "dframe.select(format_number(avg('Close'),5).alias('Average(Close)')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the max and min of the Volume column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of both the max and min of the Volume column\n",
    "dict_1 = dframe.select(max(dframe['Volume']).alias('Max_Volume')).collect()[0].asDict()\n",
    "dict_2 = dframe.select(min(dframe['Volume']).alias('Min_Volume')).collect()[0].asDict()\n",
    "\n",
    "# combining the dataframe in one 'dict_1' \n",
    "dict_1.update(dict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+\n",
      "|Max_Volume|Min_Volume|\n",
      "+----------+----------+\n",
      "|  80898100|   2094900|\n",
      "+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#put in a list stored in data\n",
    "data = [dict_1 ]\n",
    "\n",
    "# creating a dataframe\n",
    "df = spark.createDataFrame(data)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many days was the Close lower than 60 dollars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of days the Close is lower than 60 dollars is 81\n"
     ]
    }
   ],
   "source": [
    "# the number of days the Close is lower than 60 dollars\n",
    "lower = dframe.filter(\"Close < 60\").count()\n",
    "print(f\"the number of days the Close is lower than 60 dollars is {lower}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What percentage of the time was the High greater than 80 dollars ?\n",
    "#### In other words, (Number of Days High>80)/(Total Days in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count each and store in variables\n",
    "total_days = dframe.select(dayofyear(dframe['Date'])).count()\n",
    "high_gr_than_80 = dframe.filter(\"High > 80\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of time High is greater than 80 dollars is 9.141494435612083\n"
     ]
    }
   ],
   "source": [
    "# percentage calculation\n",
    "percentage = (high_gr_than_80 / total_days) * 100\n",
    "percentage\n",
    "print(f\"The percentage of time High is greater than 80 dollars is {percentage}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the Pearson correlation between High and Volume?\n",
    "#### make sure to use spark functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "| corr(High, Volume)|\n",
      "+-------------------+\n",
      "|-0.3384326061737161|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculating the Pearson correlation between High and Volume\n",
    "dframe.select(corr(\"High\", \"Volume\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the max High for each year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|Years|max(High)|\n",
      "+-----+---------+\n",
      "| 2012|77.599998|\n",
      "| 2013|81.370003|\n",
      "| 2014|88.089996|\n",
      "| 2015|90.970001|\n",
      "| 2016|75.190002|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# max values for each year\n",
    "dframe.groupBy(year(dframe['Date']).alias('Years'))\\\n",
    "    .agg({\"High\":'max'})\\\n",
    "    .sort(asc(year(dframe['Date'])))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the average Close for each  month (output should be 12 values)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------------+\n",
      "|Month|       avg(Close)|\n",
      "+-----+-----------------+\n",
      "|    1|71.44801958415842|\n",
      "|    2|  71.306804443299|\n",
      "|    3|71.77794377570092|\n",
      "|    4|72.97361900952382|\n",
      "|    5|72.30971688679247|\n",
      "|    6| 72.4953774245283|\n",
      "|    7|74.43971943925233|\n",
      "|    8|73.02981855454546|\n",
      "|    9|72.18411785294116|\n",
      "|   10|71.57854545454543|\n",
      "|   11| 72.1110893069307|\n",
      "|   12|72.84792478301885|\n",
      "+-----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#the average Close valu for each  month.\n",
    "dframe.groupBy(month(dframe['Date']).alias('Month'))\\\n",
    "    .agg({\"Close\":'avg'})\\\n",
    "    .sort(asc(month(dframe['Date'])))\\\n",
    "    .show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer the following with just a few sentences:\n",
    "#### Consider pandas. fillna() method has an argument called 'inplace'.\n",
    "#### (1) What is the purpose of that argument?\n",
    "#### (2) Do we have something similar in spark? \n",
    "#### (3) Explain your answer to (2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------\n",
    "#### ANSWERS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (1) What is the purpose of that argument?\n",
    "\n",
    "The inplace parameter in the fillna() method of Pandas DataFrame is used to specify whether to modify the DataFrame in place or return a new DataFrame with missing values filled. If inplace=True, the method will modify the original DataFrame and return None. This means that the missing values will be replaced in the original DataFrame without creating a new copy of it. \n",
    "\n",
    "This can be useful if you want to update the original DataFrame and save memory by not creating a copy. If inplace=False or not specified, the method will return a new DataFrame with the missing values filled, leaving the original DataFrame unchanged."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Do we have something similar in spark? \n",
    "\n",
    "The fillna() method in Spark DataFrame also has an inplace parameter, but it works differently from the inplace parameter in Pandas. In Spark, there is no inplace parameter, instead, the fillna() method returns a new DataFrame with the missing values filled, leaving the original DataFrame unchanged. \n",
    "\n",
    "This is because Spark DataFrames are immutable, meaning that once they are created, they cannot be modified. To update the original DataFrame in Spark, you need to assign the result of fillna() back to the original DataFrame. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (3) Explain your answer to (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id|Sales|Years|\n",
      "+----+-----+-----+\n",
      "|emp1| null|    3|\n",
      "|emp2| null| null|\n",
      "|emp3|  345|    2|\n",
      "|emp4|  456|    4|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# May take a little while on a local computer\n",
    "spark = SparkSession.builder.appName(\"filldata\").getOrCreate()\n",
    "df = spark.read.csv(\"ContainsNull1.csv\",header=True,inferSchema=True)\n",
    "df.select(['Id','Sales', 'Years']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+-----+\n",
      "|  Id| Name|Sales|Years|\n",
      "+----+-----+-----+-----+\n",
      "|emp1| John|    0|    3|\n",
      "|emp2| null|    0|    0|\n",
      "|emp3| null|  345|    2|\n",
      "|emp4|Cindy|  456|    4|\n",
      "+----+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill the missing values in the 'Sales' and 'Years' columns\n",
    "df = df.fillna(0, subset=['Sales', 'Years'])\n",
    "df.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This created a new DataFrame with missing values filled with 0 in the columns 'Sales' and 'Years', and assign the result back to the original DataFrame df.\n",
    "\n",
    "In summary, while the fillna() method in Pandas can modify the original DataFrame in place, the fillna() method in Spark always returns a new DataFrame, which needs to be assigned back to the original DataFrame to update it."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
